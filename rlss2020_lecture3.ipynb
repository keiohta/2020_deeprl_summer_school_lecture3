{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlss2020_lecture3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keiohta/2020_deeprl_summer_school_lecture3/blob/master/rlss2020_lecture3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BGPxGybVuu",
        "colab_type": "text"
      },
      "source": [
        "# 強化学習サマースクール 第3回 Model-based RL\n",
        "\n",
        "## 目次\n",
        "1. [はじめに]()\n",
        "1. [モデル予測制御]()\n",
        "1. [アンサンブルモデルによるモデルベース強化学習]()\n",
        "\n",
        "## はじめに\n",
        "第3回では、モデルベースRLとしてダイナミクス（状態遷移）モデルを用いた制御手法について取り扱います。\n",
        "本資料では、[Pendulum](https://gym.openai.com/envs/Pendulum-v0/)を題材に、基礎的なモデルベースRLのコードを読み書きすることで理論と実装両面の理解を進めることを目的とします。\n",
        "\n",
        "まず初めに、今回の演習で必要なライブラリを一度にインストールしておきましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TvSuuTt4n09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt update > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip install gym-notebook-wrapper cpprb tf2rl > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UPhEyCbaHc",
        "colab_type": "text"
      },
      "source": [
        "## モデル予測制御（MPC）\n",
        "\n",
        "MPCと一言にまとめてもたくさん種類がありますが、本演習では下記論文で提案されているものを考えます。\n",
        "- [Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/abs/1708.02596)\n",
        "\n",
        "上記論文では、ダイナミクスモデルを学習し、それを用いてRandom-sample Shooting (RS) という方法を用いて制御問題を解きます。\n",
        "\n",
        "### RS: Random-sample Shooting\n",
        "RSでは、ダイナミクスモデル $f$ と報酬（またはコスト）関数 $g$ が（正確でないにせよ）既知であると仮定し、\n",
        "それを用いて現在の状態 $s_t \\in \\mathbb{R}^{N^{state}}$ における行動 $a_t$ を生成します。\n",
        "\n",
        "ダイナミクスモデル $f$ が既知なので、次の状態を $s_{t+1} = f(s_t, a_t)$ のように予測することができます。\n",
        "更に、報酬関数を用いて $r_{t+1} = g(s_t, a_t, s_{t+1})$ からその状態において実行した行動がどれくらい良いかが得られます。\n",
        "このように、次の状態・報酬を反復的に $M$ 回計算することで、 $M$ ステップ後までの累積報酬和 $R_t^{t+M} = \\sum_{m=1}^M r_{t+m}$が得られます。\n",
        "\n",
        "RSでは、このアイデアを用いて**ランダムに**行動を $N^\\text{episode}$ エピソード分生成し、その中で最も\n",
        "累積報酬和が高かった最初の行動を選択し、1ステップ分だけエピソードを進めます。\n",
        "具体的には、以下のように進めます。\n",
        "\n",
        "1. 現在の状態を $s_t$ とし、$N^\\text{episode}$エピソード分を複製する $\\mathbf{s} \\in \\mathbb{R}^{N^\\text{episode} \\times N^{state}}$ \n",
        "1. $N^\\text{episode}$ エピソード分のランダムな行動 $\\mathbf{a} \\in \\mathbb{R}^{N^\\text{episode} \\times N^{action}}$ を生成する\n",
        "1. ダイナミクスモデルを用いて次の状態と報酬を予測する\n",
        "1. $T$ステップ先読みするまで2に戻って再度繰り返す\n",
        "1. $N^\\text{episode}$ エピソードの中で最も累積報酬が高いエピソードを選択し、その最初の行動を用いて1ステップ環境を進める\n",
        "\n",
        "### 全体の流れ\n",
        "今回扱う手法では、ダイナミクスモデルを学習しながらRSを行います。\n",
        "全体的な流れは以下のようになります。\n",
        "\n",
        "1. ランダムに遷移を収集し状態遷移モデル $f$ を事前学習\n",
        "1. $N^\\text{episode}$ エピソード分繰り返し\n",
        "  1. エピソード終端条件を満たすまで繰り返し\n",
        "    1. RSを用いて環境を進める\n",
        "    1. 得られた遷移 $(s_t, a_t, s_{t+1})$ をバッファ $\\mathcal{D}$ に保存\n",
        "  1. ダイナミクスモデル $f$ を学習\n",
        "\n",
        "それでは実装していきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBmbwStmE25U",
        "colab_type": "text"
      },
      "source": [
        "#### 環境の実装\n",
        "\n",
        "まず初めに今回の演習で使う環境であるOpenAI Gymの[Pendulum](https://gym.openai.com/envs/Pendulum-v0/)環境を用意します。\n",
        "\n",
        "Pendulumは状態数が3（現在の振り子の角度と角速度：$\\{\\sin\\theta, \\cos\\theta, \\dot{\\theta}\\}$）, 行動数が1（トルク：$\\tau$）の連続値（非離散値）入出力の中で最も簡単な環境の一つです。\n",
        "\n",
        "また、Pendulumがどのような動作をするかを可視化して確認しておきましょう。\n",
        "制御器を設計する前に環境の特徴を把握することは非常に重要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZV4A0A6FHyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gnwrapper\n",
        "import gym\n",
        " \n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "# 可視化用の環境。JupyterNotebookで可視化するためのラッパーをかましています\n",
        "monitor_env = gnwrapper.Monitor(gym.make(\"Pendulum-v0\"), size=(400, 300), directory='.', force=True,\n",
        "                                video_callable=lambda ep: True)\n",
        "episode_max_steps = 200\n",
        "\n",
        "for episode_idx in range(3):\n",
        "    monitor_env.reset()\n",
        "    total_rew = 0.\n",
        "    for _ in range(episode_max_steps):\n",
        "        act = monitor_env.action_space.sample()\n",
        "        _, rew, done, _ = monitor_env.step(act)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
        "\n",
        "monitor_env.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1N_iwGLXXn",
        "colab_type": "text"
      },
      "source": [
        "また、ここでPendulum特有の報酬関数についても実装しておきましょう。\n",
        "報酬計算部分のコードを[ここ](https://github.com/openai/gym/blob/6df1b994bae791667a556e193d2a215b8a1e397a/gym/envs/classic_control/pendulum.py#L51)から抜粋して使用します。\n",
        "基本的には、以下のような報酬関数になっています。\n",
        "\n",
        "$\n",
        "r = -\\left( \\left\\| \\theta_t - \\theta^\\text{goal}  \\right\\|^{2} + 0.1 \\times \\dot{\\theta}_t^2 + 0.001 \\times \\tau_t^2 \\right)\n",
        "$\n",
        "\n",
        "大まかには、目標角度 $\\theta^\\text{goal}$ に到達する際に、角速度と必要なトルクを最小化するような方策が最適な方策であることが分かります。\n",
        "また、**0に近いほど良い**ことも分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3geaVwLb7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def angle_normalize(x):\n",
        "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
        " \n",
        " \n",
        "def reward_fn(obses, acts):\n",
        "    is_single_input = obses.ndim == acts.ndim and acts.ndim == 1\n",
        "    if is_single_input:\n",
        "        thetas = np.arctan2(obses[1], obses[0])\n",
        "        theta_dots = obses[2]\n",
        "    else:\n",
        "        assert obses.ndim == acts.ndim == 2\n",
        "        assert obses.shape[0] == acts.shape[0]\n",
        "        acts = np.squeeze(acts)\n",
        "        thetas = np.arctan2(obses[:, 1], obses[:, 0])\n",
        "        theta_dots = obses[:, 2]\n",
        "        assert thetas.shape == theta_dots.shape == acts.shape\n",
        "\n",
        "    acts = np.clip(acts, -2, 2)\n",
        "    costs = angle_normalize(thetas) ** 2 + .1 * theta_dots ** 2 + .001 * (acts ** 2)\n",
        "\n",
        "    return -costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8lcZmNgfXFL",
        "colab_type": "text"
      },
      "source": [
        "#### ダイナミクスモデルの実装\n",
        "\n",
        "続いて、オンラインで学習するダイナミクスモデル $f_\\theta$ を実装します。\n",
        "ダイナミクスモデルはどのような関数近似器で近似しても良いですが、今回は2層のMLP (Multi-layer Perceptron) を用いましょう。\n",
        "\n",
        "一点注意が必要なのは、今回再現実装しようとしている[論文](https://arxiv.org/abs/1708.02596)では、直接次の状態を予測するのではなく、**次の状態と現在の状態との差分**を予測します。つまり、下記損失関数を最小化するようなパラメータ $\\theta$ を学習します。\n",
        "\n",
        "$\n",
        "\\mathcal{E}(\\theta)=\\frac{1}{|\\mathcal{D}|} \\sum_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}, \\mathbf{s}_{t+1}\\right) \\in \\mathcal{D}} \\frac{1}{2}\\left\\|\\left(\\mathbf{s}_{t+1}-\\mathbf{s}_{t}\\right)-f_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right\\|^{2}\n",
        "$\n",
        "\n",
        "ここで、$\\mathcal{D}$ は遷移を保存したバッファとなります。\n",
        "このような次の状態を直接予測するのではなく、その差分だけを予測する手法はよく使われるテクニックです。\n",
        "これにより出力の分散をある程度抑えられるなどのメリットがあります。\n",
        "\n",
        "それでは、実装していきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7HkYNGSfjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class DynamicsModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, units=(32, 32)):\n",
        "        super().__init__(name=\"MLP\")\n",
        "\n",
        "        self.l1 = Dense(units[0], name=\"L1\", activation=\"relu\")\n",
        "        self.l2 = Dense(units[1], name=\"L2\", activation=\"relu\")\n",
        "        self.l3 = Dense(output_dim, name=\"L3\", activation=\"linear\")\n",
        "\n",
        "        self._optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            self(tf.constant(np.zeros(shape=(1, input_dim), dtype=np.float32)))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        features = self.l1(inputs)\n",
        "        features = self.l2(features)\n",
        "        return self.l3(features)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        assert isinstance(inputs, np.ndarray)\n",
        "        assert inputs.ndim == 2\n",
        "\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            outputs = self.call(inputs)\n",
        "\n",
        "        return outputs.numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def fit(self, inputs, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicts = self(inputs)\n",
        "            loss = 0.5 * tf.reduce_mean(tf.square(labels - predicts))\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self._optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "obs_dim = env.observation_space.high.size\n",
        "act_dim = env.action_space.high.size\n",
        "\n",
        "dynamics_model = DynamicsModel(input_dim=obs_dim + act_dim, output_dim=obs_dim)\n",
        "\n",
        "\n",
        "def predict_next_state(obses, acts):\n",
        "    assert obses.shape[0] == acts.shape[0]\n",
        "    obs_diffs = dynamics_model.predict(np.concatenate([obses, acts], axis=1))\n",
        "    assert obses.shape == obs_diffs.shape\n",
        "    next_obses = obses + obs_diffs\n",
        "    return next_obses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zSiNRT5O8dk",
        "colab_type": "text"
      },
      "source": [
        "また、ダイナミクスモデルを学習する際に遷移を保存しておくバッファを実装します。\n",
        "リングバッファを自分で実装しても良いですが、今回は既存のライブラリである[cpprb](https://github.com/ymd-h/cpprb)を用います。\n",
        "バッファサイズは10Kとしてみます。\n",
        "cpprb は Prioritized Replay Buffer や N-step Replay Buffer やそれらの組合せ、また画像入力タスクのためのフレームスタックなどもサポートしているので興味のある方は試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJkbnxLXPKwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cpprb import ReplayBuffer\n",
        "\n",
        "# 10Kデータ分 (s, a, s') が保存できるリングバッファを用意します\n",
        "rb_dict = {\n",
        "    \"size\": 10000,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"next_obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape}}}\n",
        "dynamics_buffer = ReplayBuffer(**rb_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjw83rNQe3pj",
        "colab_type": "text"
      },
      "source": [
        "#### RS (Random-sample Shooting) の実装\n",
        "\n",
        "次にRSを実装します。便利なので方策を用意しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udnrf_Lee9W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomPolicy:\n",
        "    def __init__(self, max_action, act_dim):\n",
        "        self._max_action = max_action  # action の最大値\n",
        "        self._act_dim = act_dim  # action の次元数\n",
        "\n",
        "    def get_actions(self, batch_size):\n",
        "        # 一様分布からバッチサイズ分ランダムにサンプリング\n",
        "        return np.random.uniform(\n",
        "            low=-self._max_action,\n",
        "            high=self._max_action,\n",
        "            size=(batch_size, self._act_dim))\n",
        "\n",
        "policy = RandomPolicy(\n",
        "    max_action=env.action_space.high[0],\n",
        "    act_dim=env.action_space.high.size)\n",
        "\n",
        "\n",
        "def random_shooting(init_obs, n_mpc_episodes=64, horizon=20):\n",
        "    init_actions = policy.get_actions(batch_size=n_mpc_episodes)\n",
        "    total_rewards = np.zeros(shape=(n_mpc_episodes,))\n",
        "    obses = np.tile(init_obs, (n_mpc_episodes, 1))\n",
        "\n",
        "    for i in range(horizon):\n",
        "        acts = init_actions if i == 0 else policy.get_actions(batch_size=n_mpc_episodes)\n",
        "        next_obses = predict_next_state(obses, acts)\n",
        "        rewards = reward_fn(obses, acts)\n",
        "        total_rewards += rewards\n",
        "        obses = next_obses\n",
        "\n",
        "    idx = np.argmax(total_rewards)\n",
        "    return init_actions[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLOe195hSjY",
        "colab_type": "text"
      },
      "source": [
        "#### 実行スクリプト\n",
        "\n",
        "さて、いよいよRSを実行するコードを実装します。\n",
        "冒頭の「全体の流れ」をコメントで繰り返しながら記述していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGpiz72cjhfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "n_episodes = 100\n",
        "\n",
        "\n",
        "# ランダムに遷移を収集し状態遷移モデルを収集します\n",
        "for _ in range(10):\n",
        "    obs = env.reset()\n",
        "    for _ in range(200):\n",
        "        act = env.action_space.sample()\n",
        "        next_obs, _, done, _ = env.step(act)\n",
        "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "def fit_dynamics(n_iter=50):\n",
        "    mean_loss = 0.\n",
        "    for _ in range(n_iter):\n",
        "        samples = dynamics_buffer.sample(batch_size)\n",
        "        inputs = np.concatenate([samples[\"obs\"], samples[\"act\"]], axis=1)\n",
        "        labels = samples[\"next_obs\"] - samples[\"obs\"]\n",
        "        mean_loss += dynamics_model.fit(inputs, labels).numpy()\n",
        "    return mean_loss\n",
        "\n",
        "# ダイナミクスモデルを事前学習します\n",
        "fit_dynamics(n_iter=1000)\n",
        "\n",
        "\n",
        "total_steps = 0\n",
        "for episode_idx in range(n_episodes):\n",
        "    total_rew = 0.\n",
        "\n",
        "    obs = env.reset()\n",
        "    for _ in range(episode_max_steps):\n",
        "        total_steps += 1\n",
        "        act = random_shooting(obs)\n",
        "        next_obs, rew, done, _ = env.step(act)\n",
        "        dynamics_buffer.add(\n",
        "            obs=obs, act=act, next_obs=next_obs)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "        obs = next_obs\n",
        "\n",
        "    mean_loss = fit_dynamics(n_iter=100)\n",
        "    if episode_idx % 5 == 0:\n",
        "        print(\"iter={0: 3d} total reward: {1: 4.4f} mean loss: {2:.6f}\".format(episode_idx, total_rew, mean_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myGLLiKGgz4U",
        "colab_type": "text"
      },
      "source": [
        "最後に、学習済みモデルを使って振り子の動きを可視化してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN9WIu16g346",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習済みモデルでの結果が見たいので過去のビデオリストを削除します\n",
        "monitor_env.display(reset=True)\n",
        "\n",
        "for episode_idx in range(3):\n",
        "    obs = monitor_env.reset()\n",
        "    total_rew = 0.\n",
        "    for _ in range(episode_max_steps):\n",
        "        act = random_shooting(obs)\n",
        "        next_obs, rew, done, _ = monitor_env.step(act)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "        obs = next_obs\n",
        "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
        "\n",
        "monitor_env.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJmAxqND5G3",
        "colab_type": "text"
      },
      "source": [
        "学習済みモデルで振り子の振り上げがきちんとできていることが確認できましたか？\n",
        "ただ、ご覧のように性能がイマイチですね。RSは実装が容易で、少ないサンプル数で解けることがメリットとして挙げられますが、例えば次のような欠点があります。\n",
        "\n",
        "- $M$ステップ分の累積報酬和が最も良かったエピソードの**最初の**行動を選択するが、それが良いとは限らない（最初のステップは全然ダメでも2ステップ目以降が良い場合も考えられます）\n",
        "- 先読みステップ数、エピソード数の設計にドメイン知識が必要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3RGSoUy_0Sg",
        "colab_type": "text"
      },
      "source": [
        "## アンサンブルダイナミクスモデルを用いた強化学習\n",
        "モデルベースRLの利点・欠点をいくつかまとめましょう。\n",
        "- 利点\n",
        "  - サンプル効率が良い（サンプル効率：同じ性能に達するために必要な環境との相互作用数。少ない方が良い）\n",
        "  - ダイナミクスモデルがわかっているので既存のプラニングアルゴリズムが使える\n",
        "- 欠点\n",
        "  - モデル誤差が大きい（ダイナミクスモデルが十分に実際の環境を近似できていない）と性能が出ない\n",
        "\n",
        "モデルベースRLでは、現状手に入っているダイナミクスモデルを用いて問題を解きますが、モデル誤差が大きいとバイアス（真の環境との誤差）が積み重なり、最終的な性能が悪くなってしまいます。\n",
        "\n",
        "この性質は逐次的な意思決定を行う強化学習では大きな問題となります。\n",
        "2つ目の演習では、この欠点に挑戦します。\n",
        "モデル誤差低減の方法はいくつかありますが、ここでは複数のモデルを同時に用いる手法により解決を試みます。\n",
        "より具体的には、関数近似器を複数用意し、それらを効果的に用いることで方策の性能を向上させます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_doLaZboB8q_",
        "colab_type": "text"
      },
      "source": [
        "### ME-TRPO\n",
        "\n",
        "本演習では、下記論文について取り扱います。\n",
        "- [Model-Ensemble Trust-Region Policy Optimization (ICLR2018)](https://arxiv.org/abs/1802.10592)\n",
        "\n",
        "ME-TRPOでは、学習したダイナミクスモデルを**強化学習**に用います。\n",
        "具体的には、ダイナミクスモデルで**擬似的な**遷移を生成し、それを用いて方策と価値関数を最適化します。\n",
        "特徴を挙げておきます。\n",
        "\n",
        "- 実環境で収集したサンプルを用いて**複数**のダイナミクスモデルを学習する\n",
        "- **ダイナミクスモデルで生成したサンプル**を用いて方策を学習する\n",
        "\n",
        "これにより、全体として必要な実環境との相互作用数を削減することを狙いとしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H538LM2Jr1Rk",
        "colab_type": "text"
      },
      "source": [
        "### 全体の流れ\n",
        "以上のような特徴を念頭に置き、全体の流れを下記にまとめます。\n",
        "\n",
        "1. 方策 $\\pi_\\theta$、ダイナミクスモデル $f_\\phi$ の初期化\n",
        "1. repeat\n",
        "    1. 実環境 $f$ で $\\pi_\\theta$ を使ってサンプル収集\n",
        "    1. ダイナミクスモデル $f_\\phi$ の更新\n",
        "    1. repeat\n",
        "        1. $\\pi_\\theta$ と $f_\\phi$ を使って遷移データを生成\n",
        "        1. 生成された遷移データを使って $\\pi_\\theta$ を更新\n",
        "        1. 方策評価 $\\eta(\\theta; \\phi)$\n",
        "    1. until 方策評価が高い限り\n",
        "1. until 終了条件を満たすまで\n",
        "\n",
        "それでは実装していきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsziFFR7XY9t",
        "colab_type": "text"
      },
      "source": [
        "### 準備\n",
        "ME-TRPOのコア部分の実装の前に、他に必要な機能について準備します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CefESv2d5Bbq",
        "colab_type": "text"
      },
      "source": [
        "#### Pendulum環境の設定\n",
        "\n",
        "論文の設定に近くなるように環境側の設定を少し見直します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFTXk98UWO2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode_max_steps = 100\n",
        "debug = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgcRUa2uXjIr",
        "colab_type": "text"
      },
      "source": [
        "#### ダイナミクスモデル\n",
        "\n",
        "ダイナミクスモデルを**複数**定義します。\n",
        "また、ダイナミクスモデル学習用のバッファを初期化しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfu8qN_RX48-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ダイナミクスモデルを5つ用います\n",
        "n_dynamics_model = 5\n",
        "\n",
        "obs_dim = env.observation_space.high.size\n",
        "act_dim = env.action_space.high.size\n",
        "\n",
        "dynamics_models = [DynamicsModel(\n",
        "    input_dim=obs_dim + act_dim, output_dim=obs_dim) for _ in range(n_dynamics_model)]\n",
        "\n",
        "\n",
        "def predict_next_state(obses, acts, idx=None):\n",
        "    is_single_input = obses.ndim == acts.ndim and acts.ndim == 1\n",
        "    if is_single_input:\n",
        "        obses = np.expand_dims(obses, axis=0)\n",
        "        acts = np.expand_dims(acts, axis=0)\n",
        "\n",
        "    inputs = np.concatenate([obses, acts], axis=1)\n",
        "    idx = np.random.randint(n_dynamics_model) if idx is None else idx\n",
        "    obs_diffs = dynamics_models[idx].predict(inputs)\n",
        "\n",
        "    if is_single_input:\n",
        "        return (obses + obs_diffs)[0]\n",
        "    return obses + obs_diffs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hzAcivYffi",
        "colab_type": "text"
      },
      "source": [
        "#### 方策\n",
        "\n",
        "[論文](https://arxiv.org/abs/1802.10592)では[TRPO](https://arxiv.org/abs/1502.05477)を用いましたが、やや複雑なので代わりに[PPO](https://arxiv.org/abs/1707.06347)を用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_MyDGOZZ8Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf2rl.algos.ppo import PPO\n",
        "\n",
        "def define_policy():\n",
        "    return PPO(\n",
        "        state_shape=env.observation_space.shape,\n",
        "        action_dim=env.action_space.shape[0],\n",
        "        max_action=env.action_space.high[0],\n",
        "        is_discrete=False,\n",
        "        batch_size=64,\n",
        "        actor_units=(32, 32),\n",
        "        critic_units=(32, 32),\n",
        "        n_epoch=10,\n",
        "        lr_actor=3e-4,\n",
        "        lr_critic=3e-4,\n",
        "        hidden_activation_actor=\"tanh\",\n",
        "        hidden_activation_critic=\"tanh\",\n",
        "        discount=0.9,\n",
        "        lam=0.95,\n",
        "        entropy_coef=0.,\n",
        "        horizon=2048,\n",
        "        normalize_adv=True,\n",
        "        enable_gae=True,\n",
        "        gpu=0)\n",
        "\n",
        "policy = define_policy()\n",
        "\n",
        "def clip_action(action):\n",
        "    return np.clip(action, env.action_space.low, env.action_space.high)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lq1a1nsrLtf",
        "colab_type": "text"
      },
      "source": [
        "また、方策学習用のデータを格納するバッファを用意します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YL4CIFMrVnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rb_dict = {\n",
        "    \"size\": policy.horizon,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape},\n",
        "        \"done\": {},\n",
        "        \"logp\": {},\n",
        "        \"ret\": {},\n",
        "        \"adv\": {}}}\n",
        "on_policy_buffer = ReplayBuffer(**rb_dict)\n",
        "\n",
        "rb_dict = {\n",
        "    \"size\": episode_max_steps,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape},\n",
        "        \"next_obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"rew\": {},\n",
        "        \"done\": {},\n",
        "        \"logp\": {},\n",
        "        \"val\": {}}}\n",
        "episode_buffer = ReplayBuffer(**rb_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBvp0i31Xe7d",
        "colab_type": "text"
      },
      "source": [
        "方策を評価するためのコードも用意しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZQ9PHZgXmFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(total_steps, test_episodes=10):\n",
        "    avg_test_return = 0.\n",
        "    for i in range(test_episodes):\n",
        "        episode_return = 0.\n",
        "        obs = env.reset()\n",
        "        for _ in range(episode_max_steps):\n",
        "            act, _ = policy.get_action(obs, test=True)\n",
        "            next_obs, rew, _, _ = env.step(clip_action(act))\n",
        "            episode_return += rew\n",
        "            obs = next_obs\n",
        "        avg_test_return += episode_return\n",
        "    return avg_test_return / test_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVOzgTirVrQf",
        "colab_type": "text"
      },
      "source": [
        "### 実環境 $f$ での $\\pi_\\theta$ を使って遷移データを生成\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_zeTxSOV6co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_transitions_real_env():\n",
        "    obs = env.reset()\n",
        "    episode_steps = 0\n",
        "    for _ in range(policy.horizon):\n",
        "        episode_steps += 1\n",
        "        act, _ = policy.get_action(obs)\n",
        "        next_obs, *_ = env.step(clip_action(act))\n",
        "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
        "        obs = next_obs\n",
        "        if episode_steps == episode_max_steps:\n",
        "            episode_steps = 0\n",
        "            obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3yusE6gs3Wv",
        "colab_type": "text"
      },
      "source": [
        "### ダイナミクスモデルの更新\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUeJ8052tLhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_dynamics(n_iter=50):\n",
        "    mean_losses = np.zeros(shape=(n_dynamics_model,), dtype=np.float32)\n",
        "    for _ in range(n_iter):\n",
        "        samples = dynamics_buffer.sample(batch_size)\n",
        "        inputs = np.concatenate([samples[\"obs\"], samples[\"act\"]], axis=1)\n",
        "        labels = samples[\"next_obs\"] - samples[\"obs\"]\n",
        "        for i, dynamics_model in enumerate(dynamics_models):\n",
        "            mean_losses[i] += dynamics_model.fit(inputs, labels).numpy()\n",
        "    return mean_losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvHHO1iJtpM8",
        "colab_type": "text"
      },
      "source": [
        "### $\\pi_\\theta$ と $f_\\phi$ を使って遷移データを生成\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW1jNIoLtrUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf2rl.misc.discount_cumsum import discount_cumsum\n",
        "\n",
        "\n",
        "def collect_transitions_sim_env():\n",
        "    \"\"\"\n",
        "    Generate transitions using dynamics model\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    on_policy_buffer.clear()\n",
        "    n_episodes = 0\n",
        "    ave_episode_return = 0\n",
        "    while on_policy_buffer.get_stored_size() < policy.horizon:\n",
        "        obs = env.reset()\n",
        "        episode_return = 0.\n",
        "        for i in range(episode_max_steps):\n",
        "            act, logp, val = policy.get_action_and_val(obs)\n",
        "            env_act = clip_action(act)\n",
        "            if debug:\n",
        "                next_obs, rew, _, _ = env.step(env_act)\n",
        "            else:\n",
        "                next_obs = predict_next_state(obs, env_act)\n",
        "                rew = reward_fn(obs, act)[0]\n",
        "            episode_buffer.add(obs=obs, act=act, next_obs=next_obs, rew=rew,\n",
        "                               done=False, logp=logp, val=val)\n",
        "            obs = next_obs\n",
        "            episode_return += rew\n",
        "        finish_horizon(last_val=val)\n",
        "        ave_episode_return += episode_return\n",
        "        n_episodes += 1\n",
        "    return ave_episode_return / n_episodes\n",
        "\n",
        "\n",
        "def finish_horizon(last_val=0):\n",
        "    samples = episode_buffer._encode_sample(np.arange(episode_buffer.get_stored_size()))\n",
        "    rews = np.append(samples[\"rew\"], last_val)\n",
        "    vals = np.append(samples[\"val\"], last_val)\n",
        "\n",
        "    # GAE-Lambda advantage calculation\n",
        "    deltas = rews[:-1] + policy.discount * vals[1:] - vals[:-1]\n",
        "    advs = discount_cumsum(deltas, policy.discount * policy.lam)\n",
        "\n",
        "    # Rewards-to-go, to be targets for the value function\n",
        "    rets = discount_cumsum(rews, policy.discount)[:-1]\n",
        "    on_policy_buffer.add(\n",
        "        obs=samples[\"obs\"], act=samples[\"act\"], done=samples[\"done\"],\n",
        "        ret=rets, adv=advs, logp=np.squeeze(samples[\"logp\"]))\n",
        "    episode_buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZEdnKYLGyqQ",
        "colab_type": "text"
      },
      "source": [
        "### 生成された遷移データを使って $\\pi_\\theta$ を更新\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Yf5R_7G5zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy():\n",
        "    # Compute mean and std for normalizing advantage\n",
        "    samples = on_policy_buffer.get_all_transitions()\n",
        "    mean_adv = np.mean(samples[\"adv\"])\n",
        "    std_adv = np.std(samples[\"adv\"])\n",
        "\n",
        "    for _ in range(policy.n_epoch):\n",
        "        samples = on_policy_buffer._encode_sample(np.random.permutation(policy.horizon))\n",
        "        adv = (samples[\"adv\"] - mean_adv) / (std_adv + 1e-8)\n",
        "        actor_loss, critic_loss = 0., 0.\n",
        "        for idx in range(int(policy.horizon / policy.batch_size)):\n",
        "            target = slice(idx * policy.batch_size, (idx + 1) * policy.batch_size)\n",
        "            losses = policy.train(\n",
        "                states=samples[\"obs\"][target],\n",
        "                actions=samples[\"act\"][target],\n",
        "                advantages=adv[target],\n",
        "                logp_olds=samples[\"logp\"][target],\n",
        "                returns=samples[\"ret\"][target])\n",
        "            actor_loss += losses[0]\n",
        "            critic_loss += losses[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxJi96XHeS6",
        "colab_type": "text"
      },
      "source": [
        "### 方策評価 $\\eta(\\theta; \\phi)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldPJ-UjFHjcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 各ダイナミクスモデルにおいて何エピソード分評価するかを決めます\n",
        "n_eval_episodes_per_model = 5\n",
        "\n",
        "def evaluate_current_return(init_states):\n",
        "    n_episodes = n_dynamics_model * n_eval_episodes_per_model\n",
        "    assert init_states.shape[0] == n_episodes\n",
        "\n",
        "    obses = init_states.copy()\n",
        "    next_obses = np.zeros_like(obses)\n",
        "    returns = np.zeros(shape=(n_episodes,), dtype=np.float32)\n",
        "\n",
        "    for _ in range(episode_max_steps):\n",
        "        acts, _ = policy.get_action(obses)\n",
        "        for i in range(n_episodes):\n",
        "            model_idx = i // n_eval_episodes_per_model\n",
        "            env_act = np.clip(acts[i], env.action_space.low, env.action_space.high)\n",
        "            next_obses[i] = predict_next_state(obses[i], env_act, idx=model_idx)\n",
        "        returns += reward_fn(obses, acts)\n",
        "        obses = next_obses\n",
        "\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhvMxwcIM5W",
        "colab_type": "text"
      },
      "source": [
        "### 実行\n",
        "\n",
        "以上で必要なパーツは全て揃いました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHkoT0vUUgIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_steps = 0\n",
        "test_episodes = 10\n",
        "\n",
        "while True:\n",
        "    # Collect samples\n",
        "    collect_transitions_real_env()\n",
        "    total_steps += policy.horizon\n",
        "\n",
        "    # Fit dynamics models\n",
        "    fit_dynamics()\n",
        "\n",
        "    # ret_real_env, ret_sim_env = self._evaluate_model()\n",
        "    # print(\"Returns (real, sim) = ({: .3f}, {: .3f})\".format(ret_real_env, ret_sim_env))\n",
        "\n",
        "    n_updates = 0\n",
        "    init_states_for_eval = np.array([\n",
        "        env.reset() for _ in range(n_dynamics_model * n_eval_episodes_per_model)])\n",
        "    returns_before_update = evaluate_current_return(init_states_for_eval)\n",
        "    while True:\n",
        "        n_updates += 1\n",
        "\n",
        "        # Generate samples using dynamics models\n",
        "        average_return = collect_transitions_sim_env()\n",
        "\n",
        "        # Update policy\n",
        "        update_policy()\n",
        "\n",
        "        returns_after_update = evaluate_current_return(init_states_for_eval)\n",
        "        improved_ratio = np.sum(returns_after_update > returns_before_update) / (\n",
        "                    n_dynamics_model * n_eval_episodes_per_model)\n",
        "        print(\"Training total steps: {0: 7} improved ratio: {1: .2f} simulated return: {2: .4f} at n_update: {3: 2}\".format(\n",
        "            total_steps, improved_ratio, average_return, n_updates))\n",
        "        if improved_ratio < 0.7:\n",
        "            break\n",
        "        returns_before_update = returns_after_update\n",
        "\n",
        "    # Evaluate policy\n",
        "    if total_steps // policy.horizon % 10 == 0:\n",
        "        avg_test_return = evaluate_policy(total_steps, test_episodes)\n",
        "        print(\"Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes\".format(\n",
        "            total_steps, avg_test_return, test_episodes))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}