{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlss2020_lecture3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keiohta/2020_deeprl_summer_school_lecture3/blob/master/rlss2020_lecture3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BGPxGybVuu",
        "colab_type": "text"
      },
      "source": [
        "# 強化学習サマースクール 第3回 Model-based RL\n",
        "\n",
        "## 目次\n",
        "1. [はじめに]()\n",
        "1. [モデル予測制御]()\n",
        "1. [アンサンブルモデルによるモデルベース強化学習]()\n",
        "\n",
        "## はじめに\n",
        "第3回では、モデルベースRLとしてダイナミクス（状態遷移）モデルを用いた制御手法について取り扱います。\n",
        "本資料では、[Pendulum](https://gym.openai.com/envs/Pendulum-v0/)を題材に、基礎的なモデルベースRLのコードを読み書きすることで理論と実装両面の理解を進めることを目的とします。\n",
        "\n",
        "まず初めに、今回の演習で必要なライブラリを一度にインストールしておきましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TvSuuTt4n09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt update > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip install gym-notebook-wrapper cpprb tf2rl > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UPhEyCbaHc",
        "colab_type": "text"
      },
      "source": [
        "## モデル予測制御（MPC）\n",
        "\n",
        "MPCと一言にまとめてもたくさん種類がありますが、本演習では下記論文で提案されているものを考えます。\n",
        "- [Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/abs/1708.02596)\n",
        "\n",
        "上記論文では、ダイナミクスモデルを学習し、それを用いてRandom-sample Shooting (RS) という方法を用いて制御問題を解きます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEPs_QQY31Vw",
        "colab_type": "text"
      },
      "source": [
        "### RS: Random-sample Shooting\n",
        "RSでは、ダイナミクスモデル $f$ と報酬（またはコスト）関数 $g$ が（正確でないにせよ）既知であると仮定し、\n",
        "それを用いて現在の状態 $s_t \\in \\mathbb{R}^{N^{state}}$ における行動 $a_t$ を生成します。\n",
        "\n",
        "ダイナミクスモデル $f$ が既知なので、次の状態を $s_{t+1} = f(s_t, a_t)$ のように予測することができます。\n",
        "更に、報酬関数を用いて $r_{t+1} = g(s_t, a_t, s_{t+1})$ からその状態において実行した行動がどれくらい良いかが得られます。\n",
        "このように、次の状態・報酬を反復的に $M$ 回計算することで、 $M$ ステップ後までの累積報酬和 $R_t^{t+M} = \\sum_{m=1}^M r_{t+m}$が得られます。\n",
        "\n",
        "以上のアイデアをもとに**ランダムに**行動を $N^\\text{episode}$ エピソード分生成し、その中で最も累積報酬和が高かった**最初の行動**を選択し1ステップ分だけエピソードを進めます。\n",
        "具体的には、以下のように進めます。\n",
        "\n",
        "1. 現在の状態を $s_t$ とし、この状態を $N^\\text{episode}$ エピソード分複製する $\\mathbf{s} \\in \\mathbb{R}^{N^\\text{episode} \\times N^\\text{state}}$ \n",
        "1. $N^\\text{episode}$ エピソード分のランダムな行動 $\\mathbf{a} \\in \\mathbb{R}^{N^\\text{episode} \\times N^\\text{action}}$ を生成する\n",
        "1. ダイナミクスモデル $f_\\phi$ を用いて次の状態 $s_{t+1}$ と報酬 $r_{t+1}$ を計算する\n",
        "1. $T$ステップ先読みするまで2に戻る\n",
        "1. $N^\\text{episode}$ エピソードの中で最も累積報酬が高いエピソードを選択し、その最初の行動を用いて1ステップ分環境を進める\n",
        "\n",
        "ここでは、パラメータ $\\phi$ を用いて真のダイナミクス $f$ を近似するので、ダイナミクスモデルを $f_\\phi$ と表記します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCiYldy_34Hz",
        "colab_type": "text"
      },
      "source": [
        "### 全体の流れ\n",
        "今回扱う手法では、ダイナミクスモデルを学習しながらRSを行います。\n",
        "全体的な流れは以下のようになります。\n",
        "\n",
        "1. ランダムに遷移 $(s_t, a_t, s_{t+1})$ を収集しバッファ $\\mathcal{D}$ に保存\n",
        "1. ダイナミクスモデル $f_\\phi$ を事前学習\n",
        "1. $N^\\text{episode}$ エピソード分繰り返し\n",
        "  1. エピソード終端条件を満たすまで繰り返し\n",
        "    1. RSを用いて環境を1ステップ分進める\n",
        "    1. 得られた遷移 $(s_t, a_t, s_{t+1})$ をバッファ $\\mathcal{D}$ に保存\n",
        "  1. ダイナミクスモデル $f_\\phi$ を学習\n",
        "\n",
        "それでは実装していきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBmbwStmE25U",
        "colab_type": "text"
      },
      "source": [
        "#### 環境の実装\n",
        "\n",
        "まず初めに今回の演習で使う環境であるOpenAI Gymの[Pendulum](https://gym.openai.com/envs/Pendulum-v0/)環境を用意します。\n",
        "\n",
        "Pendulumは状態の次元数 $N^\\text{state}=3$（現在の振り子の角度と角速度：$\\{\\cos\\theta, \\sin\\theta, \\dot{\\theta}\\}$）, 行動の次元数 $N^\\text{action}=1$（振り子を振り上げるのに必要なトルク：$\\tau$）となっています。\n",
        "連続値（非離散値）入出力の中で最も簡単な環境の一つです。\n",
        "\n",
        "また、Pendulumがどのような動作をするかを可視化して確認しておきましょう。\n",
        "制御器を設計する前に環境の特徴を把握することは非常に重要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZV4A0A6FHyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gnwrapper\n",
        "import gym\n",
        " \n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "# 可視化用の環境。JupyterNotebookで可視化するためのラッパーをかましています\n",
        "monitor_env = gnwrapper.Monitor(gym.make(\"Pendulum-v0\"), size=(400, 300), directory='.', force=True,\n",
        "                                video_callable=lambda ep: True)\n",
        "episode_max_steps = 200\n",
        "\n",
        "for episode_idx in range(3):\n",
        "    monitor_env.reset()\n",
        "    total_rew = 0.\n",
        "    for _ in range(episode_max_steps):\n",
        "        act = monitor_env.action_space.sample()\n",
        "        _, rew, done, _ = monitor_env.step(act)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
        "\n",
        "monitor_env.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1N_iwGLXXn",
        "colab_type": "text"
      },
      "source": [
        "また、ここでPendulum特有の報酬関数についても実装しておきましょう。\n",
        "報酬計算部分のコードを[ここ](https://github.com/openai/gym/blob/6df1b994bae791667a556e193d2a215b8a1e397a/gym/envs/classic_control/pendulum.py#L51)から抜粋して使用します。\n",
        "基本的には、以下のような報酬関数になっています。\n",
        "\n",
        "$\n",
        "r = -\\left( \\left\\| \\theta_t - \\theta^\\text{goal}  \\right\\|^{2} + 0.1 \\times \\dot{\\theta}_t^2 + 0.001 \\times \\tau_t^2 \\right)\n",
        "$\n",
        "\n",
        "大まかには、目標角度 $\\theta^\\text{goal}$ に到達する際に、角速度と必要なトルクを最小化するような方策が最適な方策であることが分かります。\n",
        "また、**0に近いほど良い**ことも分かります。\n",
        "（方策評価のときの目安になるので覚えておいてください）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3geaVwLb7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def angle_normalize(x):\n",
        "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
        " \n",
        "\n",
        "def reward_fn(obses, acts):\n",
        "    is_single_input = obses.ndim == acts.ndim and acts.ndim == 1\n",
        "    if is_single_input:\n",
        "        thetas = np.arctan2(obses[1], obses[0])\n",
        "        theta_dots = obses[2]\n",
        "    else:\n",
        "        assert obses.ndim == acts.ndim == 2\n",
        "        assert obses.shape[0] == acts.shape[0]\n",
        "        acts = np.squeeze(acts)\n",
        "        thetas = np.arctan2(obses[:, 1], obses[:, 0])\n",
        "        theta_dots = obses[:, 2]\n",
        "        assert thetas.shape == theta_dots.shape == acts.shape\n",
        "\n",
        "    acts = np.clip(acts, -2, 2)\n",
        "    costs = angle_normalize(thetas) ** 2 + .1 * theta_dots ** 2 + .001 * (acts ** 2)\n",
        "\n",
        "    return -costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8lcZmNgfXFL",
        "colab_type": "text"
      },
      "source": [
        "#### ダイナミクスモデルの実装\n",
        "\n",
        "続いて、オンラインで学習するダイナミクスモデル $f_\\phi$ を実装します。\n",
        "ダイナミクスモデルはどのような関数近似器で近似しても良いですが、今回は2層のMLP (Multi-layer Perceptron) を用いましょう。\n",
        "\n",
        "一点注意が必要なのは、今回再現実装しようとしている[論文](https://arxiv.org/abs/1708.02596)では、直接次の状態を予測するのではなく、**次の状態と現在の状態との差分**を予測します。つまり、下記損失関数を最小化するようなパラメータ $\\phi$ を学習します。\n",
        "\n",
        "$\n",
        "\\mathcal{E}(\\phi)=\\frac{1}{|\\mathcal{D}|} \\sum_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}, \\mathbf{s}_{t+1}\\right) \\in \\mathcal{D}} \\frac{1}{2}\\left\\|\\left(\\mathbf{s}_{t+1}-\\mathbf{s}_{t}\\right)-f_{\\phi}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right\\|^{2}\n",
        "$\n",
        "\n",
        "ここで、$\\mathcal{D}$ は遷移を保存したバッファとなります。\n",
        "このような次の状態を直接予測するのではなく、その差分だけを予測する手法はよく使われるテクニックです。\n",
        "これにより出力の分散をある程度抑えられるなどのメリットがあります。\n",
        "\n",
        "それでは、実装していきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7HkYNGSfjfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# ダイナミクスモデル\n",
        "class DynamicsModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, units=(32, 32)):\n",
        "        super().__init__(name=\"MLP\")\n",
        "\n",
        "        # 隠れ層2層のMLP\n",
        "        self.l1 = Dense(units[0], name=\"L1\", activation=\"relu\")\n",
        "        self.l2 = Dense(units[1], name=\"L2\", activation=\"relu\")\n",
        "        self.l3 = Dense(output_dim, name=\"L3\", activation=\"linear\")\n",
        "\n",
        "        self._optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            self(tf.constant(np.zeros(shape=(1, input_dim), dtype=np.float32)))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        features = self.l1(inputs)\n",
        "        features = self.l2(features)\n",
        "        return self.l3(features)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        assert isinstance(inputs, np.ndarray)\n",
        "        assert inputs.ndim == 2\n",
        "\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            outputs = self.call(inputs)\n",
        "\n",
        "        return outputs.numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def fit(self, inputs, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicts = self(inputs)\n",
        "            loss = 0.5 * tf.reduce_mean(tf.square(labels - predicts))\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self._optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "obs_dim = env.observation_space.high.size\n",
        "act_dim = env.action_space.high.size\n",
        "\n",
        "dynamics_model = DynamicsModel(input_dim=obs_dim + act_dim, output_dim=obs_dim)\n",
        "\n",
        "\n",
        "def predict_next_state(obses, acts):\n",
        "    assert obses.shape[0] == acts.shape[0]\n",
        "    obs_diffs = dynamics_model.predict(np.concatenate([obses, acts], axis=1))\n",
        "    assert obses.shape == obs_diffs.shape\n",
        "    next_obses = obses + obs_diffs\n",
        "    return next_obses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zSiNRT5O8dk",
        "colab_type": "text"
      },
      "source": [
        "また、ダイナミクスモデルを学習する際に遷移を保存しておくバッファを実装します。\n",
        "リングバッファを自分で実装しても良いですが、今回は既存のライブラリである[cpprb](https://github.com/ymd-h/cpprb)を用います。\n",
        "バッファサイズは10Kとしてみます。\n",
        "cpprb は Prioritized Replay Buffer や N-step Replay Buffer やそれらの組合せ、また画像入力タスクのためのフレームスタックなどもサポートしているので興味のある方は試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJkbnxLXPKwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cpprb import ReplayBuffer\n",
        "\n",
        "# 10Kデータ分 (s, a, s') を保存できるリングバッファを用意します\n",
        "rb_dict = {\n",
        "    \"size\": 10000,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"next_obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape}}}\n",
        "dynamics_buffer = ReplayBuffer(**rb_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjw83rNQe3pj",
        "colab_type": "text"
      },
      "source": [
        "#### RS (Random-sample Shooting) の実装\n",
        "\n",
        "次にRSを実装します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udnrf_Lee9W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ランダム方策\n",
        "class RandomPolicy:\n",
        "    def __init__(self, max_action, act_dim):\n",
        "        self._max_action = max_action  # action の最大値\n",
        "        self._act_dim = act_dim  # action の次元数\n",
        "\n",
        "    def get_actions(self, batch_size):\n",
        "        # 一様分布からバッチサイズ分ランダムにサンプリング\n",
        "        return np.random.uniform(\n",
        "            low=-self._max_action,\n",
        "            high=self._max_action,\n",
        "            size=(batch_size, self._act_dim))\n",
        "\n",
        "policy = RandomPolicy(\n",
        "    max_action=env.action_space.high[0],\n",
        "    act_dim=env.action_space.high.size)\n",
        "\n",
        "\n",
        "# RSの実装\n",
        "def random_shooting(init_obs, n_mpc_episodes=64, horizon=20):\n",
        "    init_actions = policy.get_actions(batch_size=n_mpc_episodes)\n",
        "\n",
        "    returns = np.zeros(shape=(n_mpc_episodes,))\n",
        "    obses = np.tile(init_obs, (n_mpc_episodes, 1))\n",
        "\n",
        "    # horizon分未来まで予測\n",
        "    for i in range(horizon):\n",
        "        acts = init_actions if i == 0 else policy.get_actions(batch_size=n_mpc_episodes)\n",
        "        next_obses = predict_next_state(obses, acts)  # ダイナミクスモデルを用いた次状態の予測\n",
        "        rewards = reward_fn(obses, acts)\n",
        "        returns += rewards\n",
        "        obses = next_obses\n",
        "\n",
        "    return init_actions[np.argmax(returns)]  # 最も累積報酬が高かった最初の行動を返す"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLOe195hSjY",
        "colab_type": "text"
      },
      "source": [
        "#### 実行スクリプト\n",
        "\n",
        "さて、いよいよRSを実行するコードを実装します。\n",
        "冒頭の「全体の流れ」をコメントで繰り返しながら記述していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGpiz72cjhfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "n_episodes = 100\n",
        "\n",
        "\n",
        "# ダイナミクスモデルの学習用関数の定義\n",
        "def fit_dynamics(n_iter=50):\n",
        "    mean_loss = 0.\n",
        "    for _ in range(n_iter):\n",
        "        samples = dynamics_buffer.sample(batch_size)\n",
        "        inputs = np.concatenate([samples[\"obs\"], samples[\"act\"]], axis=1)\n",
        "        labels = samples[\"next_obs\"] - samples[\"obs\"]\n",
        "        mean_loss += dynamics_model.fit(inputs, labels).numpy()\n",
        "    return mean_loss\n",
        "\n",
        "\n",
        "# ダイナミクスモデルの事前学習のために実環境でランダムに遷移を収集\n",
        "for _ in range(10):\n",
        "    obs = env.reset()\n",
        "    for _ in range(200):\n",
        "        act = env.action_space.sample()\n",
        "        next_obs, _, done, _ = env.step(act)\n",
        "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "# ダイナミクスモデルの事前学習\n",
        "fit_dynamics(n_iter=1000)\n",
        "\n",
        "\n",
        "total_steps = 0\n",
        "for episode_idx in range(n_episodes):\n",
        "    total_rew = 0.\n",
        "\n",
        "    obs = env.reset()\n",
        "    for _ in range(episode_max_steps):\n",
        "        total_steps += 1\n",
        "\n",
        "        # RSを使って1ステップだけ進める\n",
        "        act = random_shooting(obs)\n",
        "        next_obs, rew, done, _ = env.step(act)\n",
        "\n",
        "        # 収集した遷移をバッファに保存\n",
        "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
        "\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "        obs = next_obs\n",
        "\n",
        "    # ダイナミクスモデルの更新\n",
        "    mean_loss = fit_dynamics(n_iter=100)\n",
        "\n",
        "    if episode_idx % 5 == 0:\n",
        "        print(\"iter={0: 3d} total reward: {1: 4.4f} mean loss: {2:.6f}\".format(\n",
        "            episode_idx, total_rew, mean_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myGLLiKGgz4U",
        "colab_type": "text"
      },
      "source": [
        "最後に、学習済みモデルを使って振り子の動きを可視化してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN9WIu16g346",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode_idx in range(3):\n",
        "    obs = monitor_env.reset()\n",
        "    total_rew = 0.\n",
        "    for _ in range(episode_max_steps):\n",
        "        act = random_shooting(obs)\n",
        "        next_obs, rew, done, _ = monitor_env.step(act)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "        obs = next_obs\n",
        "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
        "\n",
        "monitor_env.display(reset=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJmAxqND5G3",
        "colab_type": "text"
      },
      "source": [
        "学習済みモデルで振り子の振り上げがきちんとできていることが確認できましたか？\n",
        "ただ、ご覧のように性能がイマイチですね。RSは実装が容易で、少ないサンプル数で解けることがメリットとして挙げられますが、例えば次のような欠点があります。\n",
        "\n",
        "- $M$ ステップ分の累積報酬和が最も良かったエピソードの**最初の**行動を選択するが、それが良いとは限らない（最初のステップは非最適でも2ステップ目以降が良い場合）\n",
        "- 先読みステップ数 $M$、エピソード数 $N^\\text{episode}$ の設計にドメイン知識が必要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3RGSoUy_0Sg",
        "colab_type": "text"
      },
      "source": [
        "## アンサンブルダイナミクスモデルを用いた強化学習\n",
        "モデルベースRLの利点・欠点をいくつかまとめましょう。\n",
        "- 利点\n",
        "  - サンプル効率が良い（サンプル効率：同じ性能に達するために必要な環境との相互作用数。少ない方が良い）\n",
        "  - ダイナミクスモデルがわかっているので既存のプラニングアルゴリズムが使える\n",
        "- 欠点\n",
        "  - モデル誤差が大きい（ダイナミクスモデルが十分に実際の環境を近似できていない）と性能が出ない\n",
        "\n",
        "モデルベースRLでは、現状手に入っているダイナミクスモデルを用いて問題を解きますが、モデル誤差が大きいとバイアス（真の環境との誤差）が積み重なり、最終的な性能が悪くなってしまいます。\n",
        "\n",
        "この性質は逐次的な意思決定を行う強化学習では大きな問題となります。\n",
        "2つ目の演習では、この欠点に挑戦します。\n",
        "モデル誤差低減の方法はいくつかありますが、ここでは複数のモデルを同時に用いる手法により解決を試みます。\n",
        "より具体的には、関数近似器を複数用意し、それらを効果的に用いることで方策の性能を向上させます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_doLaZboB8q_",
        "colab_type": "text"
      },
      "source": [
        "### ME-TRPO\n",
        "\n",
        "本演習では、下記論文について取り扱います。\n",
        "- [Model-Ensemble Trust-Region Policy Optimization (ICLR2018)](https://arxiv.org/abs/1802.10592)\n",
        "\n",
        "ME-TRPOでは、学習したダイナミクスモデルを**強化学習**に用います。\n",
        "具体的には、ダイナミクスモデルで**擬似的な**遷移を生成し、それを用いて方策と価値関数を最適化します。\n",
        "以下のような特徴があります。\n",
        "\n",
        "- 実環境で収集したサンプルを用いて**複数**のダイナミクスモデルを学習する\n",
        "- **ダイナミクスモデルで生成したサンプル**を用いて方策を学習する\n",
        "\n",
        "これにより、全体として必要な実環境との相互作用数を削減することを狙いとしています。\n",
        "本演習では、論文の結果を厳密に再現することではなく、ME-TRPOの考え方を習得することを目標とします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H538LM2Jr1Rk",
        "colab_type": "text"
      },
      "source": [
        "### 全体の流れ\n",
        "以上のような特徴を念頭に置き、全体の流れを下記にまとめます。\n",
        "\n",
        "1. 方策 $\\pi_\\theta$、ダイナミクスモデル $f_\\phi$ の初期化\n",
        "1. repeat\n",
        "    1. 実環境 $f$ で $\\pi_\\theta$ を使ってサンプル収集\n",
        "    1. ダイナミクスモデル $f_\\phi$ の更新\n",
        "    1. repeat\n",
        "        1. $\\pi_\\theta$ と $f_\\phi$ を使って遷移データを生成\n",
        "        1. 生成された遷移データを使って $\\pi_\\theta$ を更新\n",
        "        1. 方策評価 $\\eta(\\theta; \\phi)$\n",
        "    1. until 方策評価が高い限り\n",
        "1. until 終了条件を満たすまで\n",
        "\n",
        "それでは実装していきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsziFFR7XY9t",
        "colab_type": "text"
      },
      "source": [
        "### 準備\n",
        "ME-TRPOのコア部分の実装の前に、他に必要な機能について準備します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CefESv2d5Bbq",
        "colab_type": "text"
      },
      "source": [
        "#### Pendulum環境の設定\n",
        "\n",
        "論文の設定に近くなるように環境側の設定を少し見直します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFTXk98UWO2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode_max_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgcRUa2uXjIr",
        "colab_type": "text"
      },
      "source": [
        "#### ダイナミクスモデル $f_\\phi$ の初期化\n",
        "\n",
        "ダイナミクスモデルを**複数**定義します。\n",
        "また、ダイナミクスモデル学習用のバッファを初期化しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfu8qN_RX48-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ダイナミクスモデルの数の設定\n",
        "n_dynamics_model = 5\n",
        "\n",
        "obs_dim = env.observation_space.high.size\n",
        "act_dim = env.action_space.high.size\n",
        "\n",
        "# 複数のダイナミクスモデルを生成\n",
        "dynamics_models = [DynamicsModel(\n",
        "    input_dim=obs_dim + act_dim, output_dim=obs_dim) for _ in range(n_dynamics_model)]\n",
        "\n",
        "# ダイナミクスモデル学習用バッファのクリア\n",
        "dynamics_buffer.clear()\n",
        "\n",
        "\n",
        "# ダイナミクスモデルを用いた次の状態予測\n",
        "def predict_next_state(obses, acts, idx=None):\n",
        "    is_single_input = obses.ndim == acts.ndim and acts.ndim == 1\n",
        "    if is_single_input:\n",
        "        obses = np.expand_dims(obses, axis=0)\n",
        "        acts = np.expand_dims(acts, axis=0)\n",
        "\n",
        "    inputs = np.concatenate([obses, acts], axis=1)\n",
        "    idx = np.random.randint(n_dynamics_model) if idx is None else idx\n",
        "    obs_diffs = dynamics_models[idx].predict(inputs)\n",
        "\n",
        "    if is_single_input:\n",
        "        return (obses + obs_diffs)[0]\n",
        "    return obses + obs_diffs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hzAcivYffi",
        "colab_type": "text"
      },
      "source": [
        "#### 方策 $\\pi_\\theta$ の初期化\n",
        "\n",
        "[論文](https://arxiv.org/abs/1802.10592)では[TRPO](https://arxiv.org/abs/1502.05477)を用いましたが、実装がやや複雑なので代わりに[PPO](https://arxiv.org/abs/1707.06347)を用います。\n",
        "PPOやTRPOの詳細については、第二回の講義資料をご確認ください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_MyDGOZZ8Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf2rl.algos.ppo import PPO\n",
        "\n",
        "def define_policy():\n",
        "    return PPO(\n",
        "        state_shape=env.observation_space.shape,\n",
        "        action_dim=env.action_space.shape[0],\n",
        "        max_action=env.action_space.high[0],\n",
        "        is_discrete=False,\n",
        "        batch_size=64,\n",
        "        actor_units=(32, 32),\n",
        "        critic_units=(32, 32),\n",
        "        n_epoch=10,\n",
        "        lr_actor=3e-4,\n",
        "        lr_critic=3e-4,\n",
        "        hidden_activation_actor=\"tanh\",\n",
        "        hidden_activation_critic=\"tanh\",\n",
        "        discount=0.9,\n",
        "        lam=0.95,\n",
        "        entropy_coef=0.,\n",
        "        horizon=2048,\n",
        "        normalize_adv=True,\n",
        "        enable_gae=True,\n",
        "        gpu=0)\n",
        "\n",
        "policy = define_policy()\n",
        "\n",
        "def clip_action(action):\n",
        "    return np.clip(action, env.action_space.low, env.action_space.high)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lq1a1nsrLtf",
        "colab_type": "text"
      },
      "source": [
        "また、方策学習用のデータを格納するバッファを用意します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YL4CIFMrVnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rb_dict = {\n",
        "    \"size\": policy.horizon,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape},\n",
        "        \"done\": {},\n",
        "        \"logp\": {},\n",
        "        \"ret\": {},\n",
        "        \"adv\": {}}}\n",
        "on_policy_buffer = ReplayBuffer(**rb_dict)\n",
        "\n",
        "rb_dict = {\n",
        "    \"size\": episode_max_steps,\n",
        "    \"default_dtype\": np.float32,\n",
        "    \"env_dict\": {\n",
        "        \"obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"act\": {\"shape\": env.action_space.shape},\n",
        "        \"next_obs\": {\"shape\": env.observation_space.shape},\n",
        "        \"rew\": {},\n",
        "        \"done\": {},\n",
        "        \"logp\": {},\n",
        "        \"val\": {}}}\n",
        "episode_buffer = ReplayBuffer(**rb_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBvp0i31Xe7d",
        "colab_type": "text"
      },
      "source": [
        "最後に、方策を評価するためのコードも用意しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZQ9PHZgXmFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 複数のエピソードで現在の方策を評価し平均リターンを返す\n",
        "def evaluate_policy(total_steps, test_episodes=10):\n",
        "    avg_test_return = 0.\n",
        "    for i in range(test_episodes):\n",
        "        episode_return = 0.\n",
        "        obs = env.reset()\n",
        "        for _ in range(episode_max_steps):\n",
        "            act, _ = policy.get_action(obs, test=True)\n",
        "            next_obs, rew, _, _ = env.step(clip_action(act))\n",
        "            episode_return += rew\n",
        "            obs = next_obs\n",
        "        avg_test_return += episode_return\n",
        "    return avg_test_return / test_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVOzgTirVrQf",
        "colab_type": "text"
      },
      "source": [
        "### 実環境 $f$ において $\\pi_\\theta$ を使って遷移データを生成\n",
        "\n",
        "方策 $\\pi_\\theta$ を用いたロールアウトにより $(s_t, a_t, s_{t+1})$ の組を収集します。\n",
        "\n",
        "繰り返しになりますが、実環境 $f$ は**ダイナミクスモデルを学習するためだけ**に用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_zeTxSOV6co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_transitions_real_env():\n",
        "    obs = env.reset()\n",
        "    episode_steps = 0\n",
        "    for _ in range(policy.horizon):\n",
        "        episode_steps += 1\n",
        "        act, _ = policy.get_action(obs)\n",
        "        next_obs, *_ = env.step(clip_action(act))\n",
        "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
        "        obs = next_obs\n",
        "        if episode_steps == episode_max_steps:\n",
        "            episode_steps = 0\n",
        "            obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3yusE6gs3Wv",
        "colab_type": "text"
      },
      "source": [
        "### ダイナミクスモデル $f_\\phi$ の更新\n",
        "\n",
        "実環境で集めたサンプルを用いたダイナミクスモデル $f_\\phi$ を更新します。\n",
        "1つ目のRSでの演習と同様ですが、ここでは**複数**のモデルを学習します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUeJ8052tLhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_dynamics(n_iter=50):\n",
        "    mean_losses = np.zeros(shape=(n_dynamics_model,), dtype=np.float32)\n",
        "    for _ in range(n_iter):\n",
        "        samples = dynamics_buffer.sample(batch_size)\n",
        "        inputs = np.concatenate([samples[\"obs\"], samples[\"act\"]], axis=1)\n",
        "        labels = samples[\"next_obs\"] - samples[\"obs\"]\n",
        "        # 複数のモデルを学習\n",
        "        for i, dynamics_model in enumerate(dynamics_models):\n",
        "            mean_losses[i] += dynamics_model.fit(inputs, labels).numpy()\n",
        "    return mean_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvHHO1iJtpM8",
        "colab_type": "text"
      },
      "source": [
        "### $\\pi_\\theta$ と $f_\\phi$ を使って遷移データを生成\n",
        "続いて、方策 $\\pi_\\theta$ を学習するためのサンプルをダイナミクスモデル $f_\\phi$ を用いて生成します。\n",
        "\n",
        "ここでは、実環境 $f$ はエピソードの初期値を与える（`init_state = env.reset()`）ためだけに使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW1jNIoLtrUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf2rl.misc.discount_cumsum import discount_cumsum\n",
        "\n",
        "\n",
        "# ダイナミクスモデルを用いた方策学習用サンプルの生成\n",
        "def collect_transitions_sim_env():\n",
        "    on_policy_buffer.clear()\n",
        "    n_episodes = 0\n",
        "    ave_episode_return = 0\n",
        "    while on_policy_buffer.get_stored_size() < policy.horizon:\n",
        "        obs = env.reset()\n",
        "        episode_return = 0.\n",
        "        for i in range(episode_max_steps):\n",
        "            act, logp, val = policy.get_action_and_val(obs)\n",
        "            # ダイナミクスモデルを用いた次状態の予測\n",
        "            next_obs = predict_next_state(obs, clip_action(act))\n",
        "            rew = reward_fn(obs, act)[0]\n",
        "            episode_buffer.add(obs=obs, act=act, next_obs=next_obs, rew=rew,\n",
        "                               done=False, logp=logp, val=val)\n",
        "            obs = next_obs\n",
        "            episode_return += rew\n",
        "        finish_horizon(last_val=val)\n",
        "        ave_episode_return += episode_return\n",
        "        n_episodes += 1\n",
        "    return ave_episode_return / n_episodes\n",
        "\n",
        "\n",
        "# PPOの学習のため、エピソード終了時に必要な計算\n",
        "def finish_horizon(last_val=0):\n",
        "    samples = episode_buffer._encode_sample(np.arange(episode_buffer.get_stored_size()))\n",
        "    rews = np.append(samples[\"rew\"], last_val)\n",
        "    vals = np.append(samples[\"val\"], last_val)\n",
        "\n",
        "    # GAE-Lambda\n",
        "    deltas = rews[:-1] + policy.discount * vals[1:] - vals[:-1]\n",
        "    advs = discount_cumsum(deltas, policy.discount * policy.lam)\n",
        "\n",
        "    # 価値関数学習の際のターゲットとなるリターンを計算\n",
        "    rets = discount_cumsum(rews, policy.discount)[:-1]\n",
        "    on_policy_buffer.add(\n",
        "        obs=samples[\"obs\"], act=samples[\"act\"], done=samples[\"done\"],\n",
        "        ret=rets, adv=advs, logp=np.squeeze(samples[\"logp\"]))\n",
        "    episode_buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZEdnKYLGyqQ",
        "colab_type": "text"
      },
      "source": [
        "### 生成された遷移データを使って $\\pi_\\theta$ を更新\n",
        "\n",
        "実環境でサンプルした遷移データではなく、ダイナミクスモデルで生成した遷移データを用いて学習します。\n",
        "今回の講義はモデルベースRLに主眼を置いているので方策の更新については第二回の講義資料をご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Yf5R_7G5zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy():\n",
        "    # 前準備としてAdvantageの平均と分散の計算\n",
        "    samples = on_policy_buffer.get_all_transitions()\n",
        "    mean_adv = np.mean(samples[\"adv\"])\n",
        "    std_adv = np.std(samples[\"adv\"])\n",
        "\n",
        "    for _ in range(policy.n_epoch):\n",
        "        samples = on_policy_buffer._encode_sample(np.random.permutation(policy.horizon))\n",
        "        adv = (samples[\"adv\"] - mean_adv) / (std_adv + 1e-8)\n",
        "        actor_loss, critic_loss = 0., 0.\n",
        "        for idx in range(int(policy.horizon / policy.batch_size)):\n",
        "            target = slice(idx * policy.batch_size, (idx + 1) * policy.batch_size)\n",
        "            policy.train(\n",
        "                states=samples[\"obs\"][target],\n",
        "                actions=samples[\"act\"][target],\n",
        "                advantages=adv[target],\n",
        "                logp_olds=samples[\"logp\"][target],\n",
        "                returns=samples[\"ret\"][target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxJi96XHeS6",
        "colab_type": "text"
      },
      "source": [
        "### 方策評価 $\\eta(\\theta; \\phi)$\n",
        "\n",
        "次に、学習した方策を **ダイナミクスモデル $f_\\phi$ を用いて**評価します。\n",
        "\n",
        "$\n",
        "\\eta(\\theta ; \\phi):=\\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^{T} r\\left(s_{t}, a_{t}\\right)\\right]\n",
        "$\n",
        "\n",
        "ここで、$\\tau=\\left(s_{0}, a_{0}, \\ldots\\right), s_{0} \\sim \\rho_{0}(\\cdot), a_{t} \\sim \\pi_{\\theta}\\left(\\cdot \\mid s_{t}\\right), s_{t+1}=f_{\\phi}\\left(s_{t}, a_{t}\\right)$となります。\n",
        "つまり、初期値のみ実環境 $f$ から生成し、それ以降はダイナミクスモデル $f_\\phi$、方策 $\\pi_\\theta$ を用います。\n",
        "\n",
        "方策は異なるダイナミクスモデル （`n_dynamics_model`） において複数エピソード分（`n_eval_episodes_per_model`）評価します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldPJ-UjFHjcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 各ダイナミクスモデルにおいて何エピソード分評価するかを決めます\n",
        "n_eval_episodes_per_model = 5\n",
        "\n",
        "\n",
        "def evaluate_current_return(init_states):\n",
        "    # 同じ初期値で評価できるように、関数内で初期値を生成せず引数として与える\n",
        "    n_episodes = n_dynamics_model * n_eval_episodes_per_model\n",
        "    assert init_states.shape[0] == n_episodes\n",
        "\n",
        "    obses = init_states.copy()\n",
        "    next_obses = np.zeros_like(obses)\n",
        "    returns = np.zeros(shape=(n_episodes,), dtype=np.float32)\n",
        "\n",
        "    for _ in range(episode_max_steps):\n",
        "        # 現在の方策を用いて行動を生成\n",
        "        acts, _ = policy.get_action(obses, test=True)\n",
        "        for i in range(n_episodes):\n",
        "            model_idx = i // n_eval_episodes_per_model\n",
        "            env_act = np.clip(acts[i], env.action_space.low, env.action_space.high)\n",
        "            next_obses[i] = predict_next_state(obses[i], env_act, idx=model_idx)\n",
        "        returns += reward_fn(obses, acts)\n",
        "        obses = next_obses\n",
        "\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhvMxwcIM5W",
        "colab_type": "text"
      },
      "source": [
        "### 実行\n",
        "\n",
        "以上で必要なパーツは全て揃いました。\n",
        "上記「全体の流れ」に沿ってメインループを実装していきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHkoT0vUUgIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_steps = 0\n",
        "test_episodes = 10\n",
        "\n",
        "while True:\n",
        "    # 実環境でダイナミクスモデルを学習するためのサンプルを収集\n",
        "    collect_transitions_real_env()\n",
        "    total_steps += policy.horizon\n",
        "\n",
        "    # ダイナミクスモデルの学習\n",
        "    fit_dynamics()\n",
        "\n",
        "    n_updates = 0\n",
        "    # 方策評価のための初期値の生成\n",
        "    init_states_for_eval = np.array([\n",
        "        env.reset() for _ in range(n_dynamics_model * n_eval_episodes_per_model)])\n",
        "\n",
        "    # 方策更新前の性能評価\n",
        "    returns_before_update = evaluate_current_return(init_states_for_eval)\n",
        "    while True:\n",
        "        n_updates += 1\n",
        "\n",
        "        # ダイナミクスモデルを用いて方策学習用のサンプルを生成\n",
        "        average_return = collect_transitions_sim_env()\n",
        "\n",
        "        # 方策更新\n",
        "        update_policy()\n",
        "\n",
        "        # 方策更新後の性能評価\n",
        "        returns_after_update = evaluate_current_return(init_states_for_eval)\n",
        "\n",
        "        # 方策更新による性能評価の割合を計算\n",
        "        improved_ratio = np.sum(returns_after_update > returns_before_update) / (\n",
        "                    n_dynamics_model * n_eval_episodes_per_model)\n",
        "        print(\"Training total steps: {0: 7} improved ratio: {1: .2f} simulated return: {2: .4f} at n_update: {3: 2}\".format(\n",
        "            total_steps, improved_ratio, average_return, n_updates))\n",
        "        \n",
        "        # 方策更新による性能向上があまり見られない場合、ループを抜ける\n",
        "        if improved_ratio < 0.7:\n",
        "            break\n",
        "        returns_before_update = returns_after_update\n",
        "\n",
        "    # 実環境での方策評価\n",
        "    if total_steps // policy.horizon % 10 == 0:\n",
        "        avg_test_return = evaluate_policy(total_steps, test_episodes)\n",
        "        print(\"Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes\".format(\n",
        "            total_steps, avg_test_return, test_episodes))\n",
        "\n",
        "    # 100回で終了とする\n",
        "    if total_steps // policy.horizon % 100 == 0:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5q00MGiDE6v",
        "colab_type": "text"
      },
      "source": [
        "### 学習済み方策を可視化し性能評価\n",
        "\n",
        "最後に、学習済みの方策 $\\pi_\\theta$ を可視化し、定性的に性能評価してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgtMMN3XDLUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode_idx in range(3):\n",
        "    obs = monitor_env.reset()\n",
        "    total_rew = 0.\n",
        "    for _ in range(episode_max_steps):\n",
        "        act, _ = policy.get_action(obs, test=True)\n",
        "        next_obs, rew, done, _ = monitor_env.step(act)\n",
        "        total_rew += rew\n",
        "        if done:\n",
        "            break\n",
        "        obs = next_obs\n",
        "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
        "\n",
        "monitor_env.display()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}